{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np  # for numerical operations.\n",
    "\n",
    "import pandas as pd  # for data manipulation and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Train Dataset\n",
    "Train_Provider = pd.read_excel('MFD Train.xlsx', sheet_name='Train_Provider')  # Load Provider Train Data.\n",
    "Train_Beneficiarydata = pd.read_excel('MFD Train.xlsx', sheet_name='Train_Beneficiary_Data')  # Load Beneficiary Train Data.\n",
    "Train_Inpatientdata = pd.read_excel('MFD Train.xlsx', sheet_name='Train_Inpatient_Data')  # Load Inpatient Train Data.\n",
    "Train_Outpatientdata = pd.read_excel('MFD Train.xlsx', sheet_name='Train_Outpatient_Data')  # Load Outpatient Train Data.\n",
    "\n",
    "# Load Test Dataset\n",
    "Test_Provider = pd.read_excel(\"MFD Test.xlsx\", sheet_name='Test_Provider')  # Load Provider Test Data.\n",
    "Test_Beneficiarydata = pd.read_excel(\"MFD Test.xlsx\", sheet_name='Test_Beneficiary_Data')  # Load Beneficiary Test Data.\n",
    "Test_Inpatientdata = pd.read_excel('MFD Test.xlsx', sheet_name='Test_Inpatient_Data')  # Load Inpatient Test Data.\n",
    "Test_Outpatientdata = pd.read_excel(\"MFD Test.xlsx\", sheet_name='Test_Outpatient_Data')  # Load Outpatient Test Data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print shapes of train dataframes\n",
    "print('Shape of Train data :', Train_Provider.shape)\n",
    "print('Shape of Train_Beneficiarydata data :', Train_Beneficiarydata.shape)  \n",
    "print('Shape of Train_Inpatientdata data :', Train_Inpatientdata.shape)  \n",
    "print('Shape of Train_Outpatientdata data :', Train_Outpatientdata.shape)  \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check basic info\n",
    "print(Test_Provider.info())\n",
    "print(Test_Beneficiarydata.info())\n",
    "print(Test_Inpatientdata.info())\n",
    "print(Test_Outpatientdata.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing values summary \n",
    "def missing_values_summary(df, name):\n",
    "    total = df.isnull().sum()\n",
    "    percent = (total / len(df)) * 100\n",
    "    missing_df = pd.DataFrame({\n",
    "        'Total Missing': total,\n",
    "        'Percent Missing': percent\n",
    "    })\n",
    "    print(f\"\\nMissing Values Summary for {name}:\")\n",
    "    print(missing_df[missing_df['Total Missing'] > 0])  \n",
    "\n",
    "# Test:\n",
    "missing_values_summary(Test_Provider, 'Test_Provider')\n",
    "missing_values_summary(Test_Beneficiarydata, 'Test_Beneficiarydata')\n",
    "missing_values_summary(Test_Inpatientdata, 'Test_Inpatientdata')\n",
    "missing_values_summary(Test_Outpatientdata, 'Test_Outpatientdata')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "missing_values_summary(Train_Provider, 'Train_Provider')\n",
    "missing_values_summary(Train_Beneficiarydata, 'Train_Beneficiarydata')\n",
    "missing_values_summary(Train_Inpatientdata, 'Train_Inpatientdata')\n",
    "missing_values_summary(Train_Outpatientdata, 'Train_Outpatientdata')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns with threshold% missing\n",
    "def drop_zero_info_cols(df, threshold=0.90, retain_cols=[]):\n",
    "\n",
    "    missing_ratio = df.isnull().mean()\n",
    "    cols_to_drop_missing = missing_ratio[missing_ratio >= threshold].index.tolist()\n",
    "    \n",
    "    # Drop zero variance columns\n",
    "    zero_var_cols = [col for col in df.columns if df[col].nunique() <= 1]\n",
    "    \n",
    "    # Combine all columns to drop\n",
    "    cols_to_drop = list(set(cols_to_drop_missing + zero_var_cols))\n",
    "    \n",
    "    # Exclude columns to retain\n",
    "    cols_to_drop = [col for col in cols_to_drop if col not in retain_cols]\n",
    "    \n",
    "    print(f\"Dropping {len(cols_to_drop)} columns: {cols_to_drop}\")\n",
    "    \n",
    "    df_cleaned = df.drop(columns=cols_to_drop)\n",
    "    \n",
    "    return df_cleaned\n",
    "\n",
    "\n",
    "retain_cols = ['OtherPhysician']\n",
    "\n",
    "train_inpatient_cleaned = drop_zero_info_cols(Train_Inpatientdata, threshold=0.90, retain_cols=retain_cols)\n",
    "train_outpatient_cleaned = drop_zero_info_cols(Train_Outpatientdata, threshold=0.90, retain_cols=retain_cols)\n",
    "\n",
    "test_inpatient_cleaned = drop_zero_info_cols(Test_Inpatientdata, threshold=0.90, retain_cols=retain_cols)\n",
    "test_outpatient_cleaned = drop_zero_info_cols(Test_Outpatientdata, threshold=0.90, retain_cols=retain_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check categorical columns\n",
    "categorical_cols_inpatient = train_inpatient_cleaned.select_dtypes(include='object').columns\n",
    "categorical_cols_outpatient = train_outpatient_cleaned.select_dtypes(include='object').columns\n",
    "\n",
    "print(\"Inpatient Categorical Columns:\", categorical_cols_inpatient)\n",
    "print(\"Outpatient Categorical Columns:\", categorical_cols_outpatient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print no of entries missing and corresponding percentages\n",
    "def check_null_empty_percentage(df, cat_cols, name=''):\n",
    "    print(f\"{name} - Missing/Empty Summary:\")\n",
    "    total = len(df)\n",
    "    for col in cat_cols:\n",
    "        nulls = df[col].isnull().sum()\n",
    "        empties = (df[col] == '').sum()\n",
    "        total_missing = nulls + empties\n",
    "        percent = (total_missing / total) * 100\n",
    "        print(f\"{col}: {total_missing} missing/empty ({percent:.2f}%)\")\n",
    "    print('-' * 50)\n",
    "\n",
    "# Train Inpatient\n",
    "check_null_empty_percentage(train_inpatient_cleaned, categorical_cols_inpatient, 'Train Inpatient')\n",
    "\n",
    "# Test Inpatient\n",
    "check_null_empty_percentage(test_inpatient_cleaned, categorical_cols_inpatient, 'Test Inpatient')\n",
    "\n",
    "# Train Outpatient\n",
    "check_null_empty_percentage(train_outpatient_cleaned, categorical_cols_outpatient, 'Train Outpatient')\n",
    "\n",
    "# Test Outpatient\n",
    "check_null_empty_percentage(test_outpatient_cleaned, categorical_cols_outpatient, 'Test Outpatient')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute(df):\n",
    "    \n",
    "    # Renal Disease Indicator Encoding\n",
    "    if 'RenalDiseaseIndicator' in df.columns:\n",
    "        df['RenalDiseaseIndicator'] = df['RenalDiseaseIndicator'].replace({'Y': 1, 'N': 0, 0: 0, 1: 1}).fillna(0).astype(int)\n",
    "    \n",
    "    # Mode imputation for Diagnosis Codes\n",
    "    diagnosis_cols = [col for col in df.columns if 'ClmDiagnosisCode' in col or 'ClmAdmitDiagnosisCode' in col]\n",
    "    for col in diagnosis_cols:\n",
    "        mode_val = df[col].mode()[0] if not df[col].mode().empty else 'Unknown'\n",
    "        df[col] = df[col].fillna(mode_val)\n",
    "    \n",
    "    # 'Unknown' for missing OperatingPhysician & OtherPhysician\n",
    "    physician_cols = ['OperatingPhysician', 'OtherPhysician']\n",
    "    for col in physician_cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].fillna('Unknown')\n",
    "    \n",
    "    # Replace empty strings in physician columns with 'Missing'\n",
    "    for col in physician_cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].replace('', 'Missing')\n",
    "    \n",
    "    # 4. AttendingPhysician, minimal missing values â€” fill with mode\n",
    "    if 'AttendingPhysician' in df.columns:\n",
    "        mode_val = df['AttendingPhysician'].mode()[0] if not df['AttendingPhysician'].mode().empty else 'Unknown'\n",
    "        df['AttendingPhysician'] = df['AttendingPhysician'].fillna(mode_val)\n",
    "    \n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column lists and imputation\n",
    "physician_cols = ['AttendingPhysician', 'OperatingPhysician', 'OtherPhysician']\n",
    "diagnosis_cols_inpatient = ['ClmDiagnosisCode_1', 'ClmDiagnosisCode_2', 'ClmDiagnosisCode_3',\n",
    "                            'ClmDiagnosisCode_4', 'ClmDiagnosisCode_5', 'ClmDiagnosisCode_6',\n",
    "                            'ClmDiagnosisCode_7', 'ClmDiagnosisCode_8', 'ClmDiagnosisCode_9']\n",
    "\n",
    "diagnosis_cols_outpatient = ['ClmDiagnosisCode_1', 'ClmDiagnosisCode_2', 'ClmDiagnosisCode_3',\n",
    "                             'ClmDiagnosisCode_4', 'ClmDiagnosisCode_5', 'ClmAdmitDiagnosisCode']\n",
    "\n",
    "# Inpatient\n",
    "train_inpatient_cleaned = impute(train_inpatient_cleaned)\n",
    "test_inpatient_cleaned = impute(test_inpatient_cleaned)\n",
    "\n",
    "# Outpatient\n",
    "train_outpatient_cleaned = impute(train_outpatient_cleaned)\n",
    "test_outpatient_cleaned = impute(test_outpatient_cleaned)\n",
    "\n",
    "\n",
    "# Beneficiary Data\n",
    "train_beneficiary_cleaned = impute(Train_Beneficiarydata)\n",
    "test_beneficiary_cleaned = impute(Test_Beneficiarydata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick glance at first few records\n",
    "Test_Provider.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Test_Beneficiarydata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_inpatient_cleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_outpatient_cleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inpatient_cleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_outpatient_cleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shapes of Test dataframes\n",
    "print('Shape of Test_Provider:', Test_Provider.shape)\n",
    "print('Shape of Test_Beneficiarydata:', test_beneficiary_cleaned.shape)\n",
    "print('Shape of test_inpatient_cleaned:', test_inpatient_cleaned.shape)\n",
    "print('Shape of test_outpatient_cleaned:', test_outpatient_cleaned.shape)\n",
    "\n",
    "# shapes of Train dataframes\n",
    "print('Shape of Train_Provider:', Train_Provider.shape)\n",
    "print('Shape of Train_Beneficiarydata:', train_beneficiary_cleaned.shape)\n",
    "print('Shape of train_inpatient_cleaned:', train_inpatient_cleaned.shape)\n",
    "print('Shape of train_outpatient_cleaned:', train_outpatient_cleaned.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shape and sample of Train data\n",
    "print(f\"Train Shape: {Train_Provider.shape}\\n\")\n",
    "print(\"Train Sample:\\n\", Train_Provider.head())\n",
    "\n",
    "# shape and sample of Test data\n",
    "print(f\"\\nTest Shape: {Test_Provider.shape}\\n\")\n",
    "print(\"Test Sample:\\n\", Test_Provider.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for duplicates\n",
    "def check_provider_duplicates(df, dataset_name):\n",
    "    if 'Provider' not in df.columns:\n",
    "        print(f\"'{dataset_name}' dataset does not have a 'Provider' column.\")\n",
    "        return\n",
    "\n",
    "    # Count provider occurrences\n",
    "    provider_counts = df['Provider'].value_counts()\n",
    "\n",
    "    # Print provider counts\n",
    "    print(f\"\\n{'='*40}\")\n",
    "    print(f\"{dataset_name} Provider Counts:\")\n",
    "    print(provider_counts)\n",
    "\n",
    "    # Check for duplicates\n",
    "    has_duplicates = (provider_counts > 1).any()\n",
    "\n",
    "    if has_duplicates:\n",
    "        print(f\"\\nDuplicates found in {dataset_name} Provider column.\")\n",
    "        print(f\"Providers with multiple entries:\")\n",
    "        print(provider_counts[provider_counts > 1])\n",
    "    else:\n",
    "        print(f\"\\nNo duplicates found in {dataset_name} Provider column.\")\n",
    "    print(f\"{'='*40}\")\n",
    "\n",
    "# Train:\n",
    "check_provider_duplicates(Train_Provider, \"Train\")\n",
    "\n",
    "# Test:\n",
    "check_provider_duplicates(Test_Provider, \"Test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping columns by dta types\n",
    "import inspect\n",
    "\n",
    "def group_columns_by_dtype(df):\n",
    "    # Get variable name\n",
    "    callers_local_vars = inspect.currentframe().f_back.f_locals.items()\n",
    "    df_name = [var_name for var_name, var_val in callers_local_vars if var_val is df]\n",
    "    df_name = df_name[0] if df_name else 'DataFrame'\n",
    "    \n",
    "    print(f\"\\n{'='*40}\")\n",
    "    print(f\"Data Types Summary for {df_name}:\")\n",
    "    print(f\"{'='*40}\\n\")\n",
    "    \n",
    "    # Get unique data types in dataframe\n",
    "    data_types = df.dtypes.unique()\n",
    "    \n",
    "    # Loop over each unique dtype\n",
    "    for dtype in data_types:\n",
    "        # Get column names for current data type\n",
    "        cols = df.select_dtypes(include=[dtype]).columns.tolist()\n",
    "        \n",
    "        print(f\"Data Type: {dtype}\")\n",
    "        print(f\"Columns ({len(cols)} columns): {cols}\\n\")\n",
    "\n",
    "# Dictionary of datasets\n",
    "datasets = {\n",
    "    \"Train_Provider\": Train_Provider,\n",
    "    \"Test_Provider\": Test_Provider,\n",
    "    \"Train_Beneficiarydata\": train_beneficiary_cleaned,\n",
    "    \"Test_Beneficiarydata\": test_beneficiary_cleaned,\n",
    "    \"Train_Inpatientdata\": train_inpatient_cleaned,\n",
    "    \"Test_Inpatientdata\": test_inpatient_cleaned,\n",
    "    \"Train_Outpatientdata\": train_outpatient_cleaned,\n",
    "    \"Test_Outpatientdata\": test_outpatient_cleaned,\n",
    "}\n",
    "\n",
    "# Loop through datasets\n",
    "for name, df in datasets.items():\n",
    "    group_columns_by_dtype(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging Datasets\n",
    "def merge_datasets(train_inpatient_cleaned, test_inpatient_cleaned,\n",
    "                   train_outpatient_cleaned, test_outpatient_cleaned,\n",
    "                   train_beneficiary_cleaned, test_beneficiary_cleaned,\n",
    "                   provider_data=None):\n",
    "\n",
    "    # Merge Inpatient with Beneficiary\n",
    "    train_inpatient_merged = pd.merge(train_inpatient_cleaned, train_beneficiary_cleaned, on='BeneID', how='left')\n",
    "    test_inpatient_merged = pd.merge(test_inpatient_cleaned, test_beneficiary_cleaned, on='BeneID', how='left')\n",
    "    \n",
    "    # Merge Outpatient with Beneficiary\n",
    "    train_outpatient_merged = pd.merge(train_outpatient_cleaned, train_beneficiary_cleaned, on='BeneID', how='left')\n",
    "    test_outpatient_merged = pd.merge(test_outpatient_cleaned, test_beneficiary_cleaned, on='BeneID', how='left')\n",
    "    \n",
    "    # Combine Inpatient and Outpatient for Train and Test separately\n",
    "    train_combined = pd.concat([train_inpatient_merged, train_outpatient_merged], axis=0, ignore_index=True)\n",
    "    test_combined = pd.concat([test_inpatient_merged, test_outpatient_merged], axis=0, ignore_index=True)\n",
    "    \n",
    "    # Optionally merge with Provider data (only applicable for Train)\n",
    "    if provider_data is not None:\n",
    "        train_combined = pd.merge(train_combined, provider_data, on='Provider', how='left')\n",
    "    \n",
    "    return train_combined, test_combined\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function\n",
    "Train_Combined, Test_Combined = merge_datasets(\n",
    "    train_inpatient_cleaned, test_inpatient_cleaned,\n",
    "    train_outpatient_cleaned, test_outpatient_cleaned,\n",
    "    train_beneficiary_cleaned, test_beneficiary_cleaned,\n",
    "    Train_Provider   # Only provider data for Train, leave Test without provider\n",
    ")\n",
    "# Check outputs\n",
    "print(f\"Train Combined Shape: {Train_Combined.shape}\")\n",
    "print(f\"Test Combined Shape: {Test_Combined.shape}\")\n",
    "\n",
    "Train_Combined.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Test_Combined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Train\n",
    "print(f\"Number of duplicate rows in Train: {Train_Combined.duplicated().sum()}\")\n",
    "\n",
    "# For Test\n",
    "print(f\"Number of duplicate rows in Test: {Test_Combined.duplicated().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Date format\n",
    "date_cols = ['ClaimStartDt', 'ClaimEndDt', 'DOB', 'DOD']\n",
    "for col in date_cols:\n",
    "    if col in df.columns:\n",
    "        df[col] = pd.to_datetime(df[col], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Test_Combined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_Combined.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding\n",
    "cols = ['AttendingPhysician', 'OperatingPhysician', 'OtherPhysician', 'Provider']\n",
    "\n",
    "for col in cols:\n",
    "    print(f\"\\nColumn: {col}\")\n",
    "    print(f\"Unique values count: {Train_Combined[col].nunique()}\")\n",
    "    print(f\"Top 5 most frequent values:\\n{Train_Combined[col].value_counts().head()}\")\n",
    "    print(f\"Number of missing/unknown: {(Train_Combined[col] == 'Unknown').sum() + (Train_Combined[col] == 'Missing').sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{col} Unique Values - Train: {Train_Combined[col].nunique()}, Test: {Test_Combined[col].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label Encoding (for ordered categories or IDs like Physicians, Providers)\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_cols = ['AttendingPhysician', 'OperatingPhysician', 'OtherPhysician', 'Provider']\n",
    "\n",
    "for col in label_cols:\n",
    "    le = LabelEncoder()\n",
    "    \n",
    "    # Combine Train & Test column values to fit encoder\n",
    "    combined_vals = pd.concat([Train_Combined[col], Test_Combined[col]]).astype(str)\n",
    "    le.fit(combined_vals)\n",
    "    \n",
    "    # Transform both datasets\n",
    "    Train_Combined[col] = le.transform(Train_Combined[col].astype(str))\n",
    "    Test_Combined[col] = le.transform(Test_Combined[col].astype(str))\n",
    "\n",
    "print(\"Label Encoding completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Train_Combined.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Test_Combined.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding\n",
    "categorical_cols = ['Gender', 'Race', 'RenalDiseaseIndicator', 'PotentialFraud']\n",
    "\n",
    "# For Train Data\n",
    "cols_to_encode_train = [col for col in categorical_cols if col in Train_Combined.columns]\n",
    "Train_Combined = pd.get_dummies(Train_Combined, columns=cols_to_encode_train, drop_first=True)\n",
    "\n",
    "# For Test Data\n",
    "cols_to_encode_test = [col for col in categorical_cols if col in Test_Combined.columns]\n",
    "Test_Combined = pd.get_dummies(Test_Combined, columns=cols_to_encode_test, drop_first=True)\n",
    "\n",
    "# Align columns\n",
    "Train_Combined, Test_Combined = Train_Combined.align(Test_Combined, join='left', axis=1, fill_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_cols_remaining = ['State', 'County', 'DiagnosisGroupCode', 'ClmAdmitDiagnosisCode',\n",
    "                        'ClmDiagnosisCode_1', 'ClmDiagnosisCode_2', 'ClmDiagnosisCode_3',\n",
    "                        'ClmDiagnosisCode_4', 'ClmDiagnosisCode_5', 'ClmDiagnosisCode_6',\n",
    "                        'ClmDiagnosisCode_7', 'ClmDiagnosisCode_8', 'ClmDiagnosisCode_9',\n",
    "                        'ClmProcedureCode_1', 'ClmProcedureCode_2']\n",
    "\n",
    "for col in label_cols_remaining:\n",
    "    le = LabelEncoder()\n",
    "    \n",
    "    # Combine Train & Test values for consistency\n",
    "    combined_vals = pd.concat([Train_Combined[col], Test_Combined[col]]).astype(str)\n",
    "    le.fit(combined_vals)\n",
    "    \n",
    "    # Transform both datasets\n",
    "    Train_Combined[col] = le.transform(Train_Combined[col].astype(str))\n",
    "    Test_Combined[col] = le.transform(Test_Combined[col].astype(str))\n",
    "\n",
    "print(\"Remaining Label Encoding completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Claimm duration\n",
    "Train_Combined['ClaimStartDt'] = pd.to_datetime(Train_Combined['ClaimStartDt'])\n",
    "Train_Combined['ClaimEndDt'] = pd.to_datetime(Train_Combined['ClaimEndDt'])\n",
    "Test_Combined['ClaimStartDt'] = pd.to_datetime(Test_Combined['ClaimStartDt'])\n",
    "Test_Combined['ClaimEndDt'] = pd.to_datetime(Test_Combined['ClaimEndDt'])\n",
    "\n",
    "Train_Combined['Claim_Duration'] = (Train_Combined['ClaimEndDt'] - Train_Combined['ClaimStartDt']).dt.days\n",
    "Test_Combined['Claim_Duration'] = (Test_Combined['ClaimEndDt'] - Test_Combined['ClaimStartDt']).dt.days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length of Stay (Inpatient specific)\n",
    "Train_Combined['AdmissionDt'] = pd.to_datetime(Train_Combined['AdmissionDt'], errors='coerce')\n",
    "Train_Combined['DischargeDt'] = pd.to_datetime(Train_Combined['DischargeDt'], errors='coerce')\n",
    "Test_Combined['AdmissionDt'] = pd.to_datetime(Test_Combined['AdmissionDt'], errors='coerce')\n",
    "Test_Combined['DischargeDt'] = pd.to_datetime(Test_Combined['DischargeDt'], errors='coerce')\n",
    "\n",
    "Train_Combined['Length_of_Stay'] = (Train_Combined['DischargeDt'] - Train_Combined['AdmissionDt']).dt.days\n",
    "Test_Combined['Length_of_Stay'] = (Test_Combined['DischargeDt'] - Test_Combined['AdmissionDt']).dt.days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Age\n",
    "Train_Combined['DOB'] = pd.to_datetime(Train_Combined['DOB'], errors='coerce')\n",
    "Test_Combined['DOB'] = pd.to_datetime(Test_Combined['DOB'], errors='coerce')\n",
    "\n",
    "Train_Combined['Age'] = Train_Combined['ClaimStartDt'].dt.year - Train_Combined['DOB'].dt.year\n",
    "Test_Combined['Age'] = Test_Combined['ClaimStartDt'].dt.year - Test_Combined['DOB'].dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deceased Flag\n",
    "Train_Combined['DOD'] = pd.to_datetime(Train_Combined['DOD'], errors='coerce')\n",
    "Test_Combined['DOD'] = pd.to_datetime(Test_Combined['DOD'], errors='coerce')\n",
    "\n",
    "Train_Combined['Deceased_Flag'] = (Train_Combined['DOD'].notnull()) & (Train_Combined['ClaimStartDt'] > Train_Combined['DOD'])\n",
    "Train_Combined['Deceased_Flag'] = Train_Combined['Deceased_Flag'].astype(int)\n",
    "\n",
    "Test_Combined['Deceased_Flag'] = (Test_Combined['DOD'].notnull()) & (Test_Combined['ClaimStartDt'] > Test_Combined['DOD'])\n",
    "Test_Combined['Deceased_Flag'] = Test_Combined['Deceased_Flag'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Claim and Month/Year\n",
    "Train_Combined['Claim_Month'] = Train_Combined['ClaimStartDt'].dt.month\n",
    "Train_Combined['Claim_Year'] = Train_Combined['ClaimStartDt'].dt.year\n",
    "\n",
    "Test_Combined['Claim_Month'] = Test_Combined['ClaimStartDt'].dt.month\n",
    "Test_Combined['Claim_Year'] = Test_Combined['ClaimStartDt'].dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of Diagnoses & Procedures per Claim\n",
    "Train_Combined['Num_Diagnoses'] = Train_Combined[[f'ClmDiagnosisCode_{i}' for i in range(1,10)]].notnull().sum(axis=1)\n",
    "Test_Combined['Num_Diagnoses'] = Test_Combined[[f'ClmDiagnosisCode_{i}' for i in range(1,10)]].notnull().sum(axis=1)\n",
    "\n",
    "Train_Combined['Num_Procedures'] = Train_Combined[[f'ClmProcedureCode_{i}' for i in range(1,3)]].notnull().sum(axis=1)\n",
    "Test_Combined['Num_Procedures'] = Test_Combined[[f'ClmProcedureCode_{i}' for i in range(1,3)]].notnull().sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chronic conditions sum\n",
    "chronic_cols = [col for col in Train_Combined.columns if 'ChronicCond_' in col]\n",
    "\n",
    "Train_Combined['Chronic_Cond_Sum'] = Train_Combined[chronic_cols].sum(axis=1)\n",
    "Test_Combined['Chronic_Cond_Sum'] = Test_Combined[chronic_cols].sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average Deductible/Reimbursement Amount per Claim\n",
    "Train_Combined['Avg_Deductible'] = (Train_Combined['IPAnnualDeductibleAmt'] + Train_Combined['OPAnnualDeductibleAmt']) / 2\n",
    "Test_Combined['Avg_Deductible'] = (Test_Combined['IPAnnualDeductibleAmt'] + Test_Combined['OPAnnualDeductibleAmt']) / 2\n",
    "\n",
    "Train_Combined['Avg_Reimbursement'] = (Train_Combined['IPAnnualReimbursementAmt'] + Train_Combined['OPAnnualReimbursementAmt']) / 2\n",
    "Test_Combined['Avg_Reimbursement'] = (Test_Combined['IPAnnualReimbursementAmt'] + Test_Combined['OPAnnualReimbursementAmt']) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique Beneficiary Count of Claims\n",
    "beneficiary_claim_count = Train_Combined['BeneID'].value_counts().to_dict()\n",
    "Train_Combined['Bene_Claim_Count'] = Train_Combined['BeneID'].map(beneficiary_claim_count)\n",
    "Test_Combined['Bene_Claim_Count'] = Test_Combined['BeneID'].map(lambda x: beneficiary_claim_count.get(x, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Previous fraud indicator\n",
    "provider_fraud = Train_Combined.groupby('Provider')['PotentialFraud_Yes'].max()\n",
    "Train_Combined['Provider_Prev_Fraud'] = Train_Combined['Provider'].map(provider_fraud)\n",
    "Test_Combined['Provider_Prev_Fraud'] = Test_Combined['Provider'].map(lambda x: provider_fraud.get(x, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encoding for claim month\n",
    "Train_Combined = pd.get_dummies(Train_Combined, columns=['Claim_Month'], prefix='Month', drop_first=True)\n",
    "Test_Combined = pd.get_dummies(Test_Combined, columns=['Claim_Month'], prefix='Month', drop_first=True)\n",
    "\n",
    "Train_Combined, Test_Combined = Train_Combined.align(Test_Combined, join='left', axis=1, fill_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping the leakage feature\n",
    "if 'ClaimStartDt' in Train_Combined.columns:\n",
    "    Train_Combined['Claim_Month'] = Train_Combined['ClaimStartDt'].dt.month\n",
    "if 'ClaimStartDt' in Test_Combined.columns:\n",
    "    Test_Combined['Claim_Month'] = Test_Combined['ClaimStartDt'].dt.month\n",
    "\n",
    "# Columns to drop (dates + identifiers + high-leakage features)\n",
    "cols_to_drop = [\n",
    "    'ClaimStartDt', 'ClaimEndDt', 'AdmissionDt', 'DischargeDt',\n",
    "    'DOB', 'DOD', 'ClaimID', 'BeneID', \n",
    "    'Provider_Prev_Fraud'  # <--- Dropping the leakage feature\n",
    "]\n",
    "\n",
    "# Drop from Train & Test if columns exist\n",
    "Train_Combined.drop(columns=[col for col in cols_to_drop if col in Train_Combined.columns], inplace=True)\n",
    "Test_Combined.drop(columns=[col for col in cols_to_drop if col in Test_Combined.columns], inplace=True)\n",
    "\n",
    "print(\"Date, ID, and leakage columns dropped successfully!\")\n",
    "\n",
    "# Encoding 'Claim_Month' (recommended)\n",
    "if 'Claim_Month' in Train_Combined.columns:\n",
    "    Train_Combined = pd.get_dummies(Train_Combined, columns=['Claim_Month'], prefix='Month', drop_first=True)\n",
    "if 'Claim_Month' in Test_Combined.columns:\n",
    "    Test_Combined = pd.get_dummies(Test_Combined, columns=['Claim_Month'], prefix='Month', drop_first=True)\n",
    "\n",
    "# Align columns after encoding\n",
    "Train_Combined, Test_Combined = Train_Combined.align(Test_Combined, join='left', axis=1, fill_value=0)\n",
    "\n",
    "print(\"Claim_Month encoding completed!\")\n",
    "\n",
    "# Save to new CSV files\n",
    "Train_Combined.to_csv('Train_Cleaned_Encoded.csv', index=False)\n",
    "Test_Combined.to_csv('Test_Cleaned_Encoded.csv', index=False)\n",
    "\n",
    "print(\"New cleaned & encoded files saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load encoded cleaned data\n",
    "train_df = pd.read_csv('Train_Cleaned_Encoded.csv')\n",
    "test_df = pd.read_csv('Test_Cleaned_Encoded.csv')\n",
    "\n",
    "# Separate target\n",
    "y = train_df['PotentialFraud_Yes']  # Assuming binary 0/1\n",
    "X_train = train_df.drop('PotentialFraud_Yes', axis=1)\n",
    "X_test = test_df.copy()  # No target column in test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load cleaned & encoded datasets\n",
    "train_df = pd.read_csv('Train_Cleaned_Encoded.csv')\n",
    "test_df = pd.read_csv('Test_Cleaned_Encoded.csv')\n",
    "\n",
    "# Separate target variable\n",
    "y = train_df['PotentialFraud_Yes']\n",
    "X_train = train_df.drop('PotentialFraud_Yes', axis=1)\n",
    "\n",
    "# Remove 'PotentialFraud_Yes' from test if it exists\n",
    "if 'PotentialFraud_Yes' in test_df.columns:\n",
    "    X_test = test_df.drop('PotentialFraud_Yes', axis=1)\n",
    "else:\n",
    "    X_test = test_df.copy()\n",
    "\n",
    "# Normalization\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"Normalization successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "\n",
    "# Split for validation\n",
    "X_tr, X_val, y_tr, y_val = train_test_split(X_train_scaled, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "# Train Random Forest (as an example)\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_tr, y_tr)\n",
    "\n",
    "# Evaluate\n",
    "y_pred = rf.predict(X_val)\n",
    "print(classification_report(y_val, y_pred))\n",
    "print(\"ROC AUC Score:\", roc_auc_score(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Feature Importance\n",
    "feature_importances = pd.Series(rf.feature_importances_, index=X_train.columns)\n",
    "feature_importances.nlargest(20).plot(kind='barh')\n",
    "plt.title(\"Top 20 Important Features\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict\n",
    "test_preds = rf.predict(X_test_scaled)\n",
    "\n",
    "# Prepare submission (no IDs, so index-based submission)\n",
    "submission = pd.DataFrame({\n",
    "    'Index': test_df.index,\n",
    "    'PotentialFraud': ['Yes' if pred == 1 else 'No' for pred in test_preds]\n",
    "})\n",
    "\n",
    "submission.to_csv('Final_Submission_Normalized.csv', index=False)\n",
    "print(\"Submission file saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Impute missing values with the median of each column\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "\n",
    "# Create a pipeline with imputer and logistic regression\n",
    "model_lr = make_pipeline(imputer, LogisticRegression(max_iter=1000))\n",
    "\n",
    "# Perform cross-validation with the imputed data\n",
    "cv_scores_lr = cross_val_score(model_lr, X_train_scaled, y, cv=skf, scoring=scoring, n_jobs=-1, verbose=1)\n",
    "\n",
    "print(f\"Logistic Regression - Cross-Validated F1 Scores: {cv_scores_lr}\")\n",
    "print(f\"Logistic Regression - Mean F1 Score: {cv_scores_lr.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install xgboost\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.metrics import make_scorer, f1_score\n",
    "\n",
    "# Define the XGBoost model with class_weight='balanced' for handling class imbalance\n",
    "model_xgb = XGBClassifier(scale_pos_weight=1, random_state=42, use_label_encoder=False, eval_metric='logloss')\n",
    "\n",
    "# Stratified K-Fold Cross-Validation\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Define the scoring metric\n",
    "scoring = make_scorer(f1_score, pos_label=True)\n",
    "\n",
    "# Perform cross-validation\n",
    "cv_scores_xgb = cross_val_score(model_xgb, X_train_scaled, y, cv=skf, scoring=scoring, n_jobs=-1)\n",
    "\n",
    "# Print the results\n",
    "print(f\"XGBoost - Cross-Validated F1 Scores: {cv_scores_xgb}\")\n",
    "print(f\"XGBoost - Mean F1 Score: {cv_scores_xgb.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.metrics import make_scorer, f1_score\n",
    "\n",
    "# Define the LightGBM model with class_weight='balanced' for handling class imbalance\n",
    "model_lgb = lgb.LGBMClassifier(class_weight='balanced', random_state=42)\n",
    "\n",
    "# Stratified K-Fold Cross-Validation\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Define the scoring metric\n",
    "scoring = make_scorer(f1_score, pos_label=True)\n",
    "\n",
    "# Perform cross-validation\n",
    "cv_scores_lgb = cross_val_score(model_lgb, X_train_scaled, y, cv=skf, scoring=scoring, n_jobs=-1)\n",
    "\n",
    "# Print the results\n",
    "print(f\"LightGBM - Cross-Validated F1 Scores: {cv_scores_lgb}\")\n",
    "print(f\"LightGBM - Mean F1 Score: {cv_scores_lgb.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confussion matrix\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# Create confusion matrix\n",
    "cm = confusion_matrix(y_val, y_pred)\n",
    "\n",
    "# Display confusion matrix\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['False', 'True'])\n",
    "disp.plot(cmap='Blues')\n",
    "plt.title(\"Random Forest Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision Recall Curve\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "y_scores = rf.predict_proba(X_val)[:, 1]\n",
    "precisions, recalls, thresholds = precision_recall_curve(y_val, y_scores)\n",
    "\n",
    "plt.plot(thresholds, precisions[:-1], label='Precision')\n",
    "plt.plot(thresholds, recalls[:-1], label='Recall')\n",
    "plt.xlabel('Threshold')\n",
    "plt.legend()\n",
    "plt.title('Precision-Recall vs Threshold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "\n",
    "# Print classification report\n",
    "print(\"Classification Report:\\n\", classification_report(y_val, y_pred))\n",
    "\n",
    "# Print ROC AUC score\n",
    "print(f\"ROC AUC Score: {roc_auc_score(y_val, y_pred):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "specificity = tn / (tn + fp)\n",
    "sensitivity = tp / (tp + fn)\n",
    "\n",
    "print(f\"Specificity (True Negative Rate): {specificity:.2f}\")\n",
    "print(f\"Sensitivity (Recall / True Positive Rate): {sensitivity:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ROC and AUC Curve\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_val, y_scores)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate (Recall)')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision-Recall Curve\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "\n",
    "precision, recall, thresholds = precision_recall_curve(y_val, y_scores)\n",
    "average_precision = average_precision_score(y_val, y_scores)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(recall, precision, lw=2, label=f'AP = {average_precision:.2f}')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
